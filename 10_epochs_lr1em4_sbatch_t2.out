'AVA-AVD.SpeakerDiarization.data' found in /scratch/map22-share/pyav/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.
'AVA-AVD.SpeakerDiarization.data' found in /scratch/map22-share/pyav/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.
***************ORIGINAL MODEL****************
Hparams
"linear":       {'hidden_size': 128, 'num_layers': 2}
"lstm":         {'hidden_size': 128, 'num_layers': 4, 'bidirectional': True, 'monolithic': True, 'dropout': 0.0, 'batch_first': True}
"num_channels": 1
"sample_rate":  16000
"sincnet":      {'stride': 10, 'sample_rate': 16000}
***************MODIFIED MODEL****************
Protocol AVA-AVD.SpeakerDiarization.data does not precompute the output of torchaudio.info(): adding a 'torchaudio.info' preprocessor for you to speed up dataloaders. See pyannote.database documentation on how to do that yourself.
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name              â”ƒ Type       â”ƒ Params â”ƒ Mode  â”ƒ   In sizes â”ƒ Out sizes â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ sincnet           â”‚ SincNet    â”‚ 42.6 K â”‚ train â”‚     [1, 1, â”‚   [1, 60, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚    160000] â”‚      589] â”‚
â”‚ 1 â”‚ lstm              â”‚ LSTM       â”‚  1.4 M â”‚ train â”‚   [1, 589, â”‚ [[1, 589, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚        60] â”‚     256], â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚            â”‚   [[8, 1, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚            â”‚ 128], [8, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚            â”‚ 1, 128]]] â”‚
â”‚ 2 â”‚ linear            â”‚ ModuleList â”‚ 49.4 K â”‚ train â”‚          ? â”‚         ? â”‚
â”‚ 3 â”‚ classifier        â”‚ Linear     â”‚    903 â”‚ train â”‚   [1, 589, â”‚  [1, 589, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚       128] â”‚        7] â”‚
â”‚ 4 â”‚ activation        â”‚ LogSoftmax â”‚      0 â”‚ train â”‚   [1, 589, â”‚  [1, 589, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚         7] â”‚        7] â”‚
â”‚ 5 â”‚ second_input      â”‚ Sequential â”‚ 61.5 K â”‚ train â”‚   [1, 300, â”‚  [1, 300, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚      1024] â”‚       60] â”‚
â”‚ 6 â”‚ merge             â”‚ Sequential â”‚  7.3 K â”‚ train â”‚   [1, 589, â”‚  [1, 589, â”‚
â”‚   â”‚                   â”‚            â”‚        â”‚       â”‚       120] â”‚       60] â”‚
â”‚ 7 â”‚ powerset          â”‚ Powerset   â”‚      0 â”‚ train â”‚          ? â”‚         ? â”‚
â”‚ 8 â”‚ validation_metric â”‚ MetricColâ€¦ â”‚      0 â”‚ train â”‚          ? â”‚         ? â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 1.5 M                                                         
Non-trainable params: 0                                                         
Total params: 1.5 M                                                             
Total estimated model params size (MB): 6                                       
Modules in train mode: 33                                                       
Modules in eval mode: 0                                                         
Epoch 9/9  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 218/218 0:02:04 â€¢ 0:00:00 1.89it/s v_num: jzix      
                                                               DiarizationErrorâ€¦
                                                               0.507            
                                                               DiarizationErrorâ€¦
                                                               0.152            
                                                               DiarizationErrorâ€¦
                                                               0.076            
                                                               DiarizationErrorâ€¦
                                                               0.279            
Hparams
"linear":       {'hidden_size': 128, 'num_layers': 2}
"lstm":         {'hidden_size': 128, 'num_layers': 4, 'bidirectional': True, 'monolithic': True, 'dropout': 0.0, 'batch_first': True}
"num_channels": 1
"sample_rate":  16000
"sincnet":      {'stride': 10, 'sample_rate': 16000}
Best clustering threshold so far: 1.3140756195361614
Best clustering threshold so far: 1.6862666453522184
Best clustering threshold so far: 1.6862666453522184
Best clustering threshold so far: 0.47014329869868554
Best clustering threshold so far: 0.47014329869868554
Best clustering threshold so far: 0.47014329869868554
Best clustering threshold so far: 0.47014329869868554
Best clustering threshold so far: 0.1292244952729873
Best clustering threshold so far: 0.1292244952729873
Best clustering threshold so far: 0.1292244952729873
Best clustering threshold so far: 0.7379731640138844
Best clustering threshold so far: 0.7379731640138844
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
Best clustering threshold so far: 0.6497162098083837
1j20qq1JyX4_c_01
1j20qq1JyX4_c_02
1j20qq1JyX4_c_03
2qQs3Y9OJX0_c_01
2qQs3Y9OJX0_c_02
2qQs3Y9OJX0_c_03
4ZpjKfu6Cl8_c_01
4ZpjKfu6Cl8_c_02
4ZpjKfu6Cl8_c_03
5milLu-6bWI_c_01
5milLu-6bWI_c_02
5milLu-6bWI_c_03
7YpF6DntOYw_c_01
7YpF6DntOYw_c_02
7YpF6DntOYw_c_03
BCiuXAuCKAU_c_01
BCiuXAuCKAU_c_02
BCiuXAuCKAU_c_03
HKjR70GCRPE_c_01
HKjR70GCRPE_c_02
HKjR70GCRPE_c_03
IKdBLciu_-A_c_01
IKdBLciu_-A_c_02
IKdBLciu_-A_c_03
KHHgQ_Pe4cI_c_01
KHHgQ_Pe4cI_c_02
KHHgQ_Pe4cI_c_03
PmElx9ZVByw_c_01
PmElx9ZVByw_c_02
PmElx9ZVByw_c_03
a5mEmM6w_ks_c_01
a5mEmM6w_ks_c_02
a5mEmM6w_ks_c_03
kMy-6RtoOVU_c_01
kMy-6RtoOVU_c_02
kMy-6RtoOVU_c_03
qrkff49p4E4_c_01
qrkff49p4E4_c_02
qrkff49p4E4_c_03
zC5Fh2tTS1U_c_01
zC5Fh2tTS1U_c_02
zC5Fh2tTS1U_c_03
zR725veL-DI_c_01
zR725veL-DI_c_02
zR725veL-DI_c_03
IzvOYVMltkI_c_01
IzvOYVMltkI_c_02
IzvOYVMltkI_c_03
UrsCy6qIGoo_c_01
UrsCy6qIGoo_c_02
UrsCy6qIGoo_c_03
yn9WN9lsHRE_c_01
yn9WN9lsHRE_c_02
yn9WN9lsHRE_c_03
                 diarization error rate   total correct correct false alarm false alarm missed detection missed detection confusion confusion
                                      %                       %                       %                                 %                   %
item                                                                                                                                         
1j20qq1JyX4_c_01                 324.12  155.05   90.89   58.62      438.39      282.74             1.20             0.78     62.96     40.61
1j20qq1JyX4_c_02                 195.63  227.81  146.61   64.36      364.47      159.99             0.03             0.01     81.17     35.63
1j20qq1JyX4_c_03                 244.99  197.26  110.43   55.98      396.43      200.97             1.45             0.73     85.38     43.28
2qQs3Y9OJX0_c_01                 603.39   89.26   53.72   60.18      503.02      563.57             0.03             0.03     35.51     39.78
2qQs3Y9OJX0_c_02                 466.43  116.06   50.93   43.88      476.21      410.31             0.03             0.03     65.10     56.09
2qQs3Y9OJX0_c_03                 324.65  147.43  111.66   75.74      442.86      300.39             0.03             0.02     35.74     24.24
4ZpjKfu6Cl8_c_01                 373.46  141.59   64.09   45.26      451.29      318.72             0.64             0.46     76.86     54.28
4ZpjKfu6Cl8_c_02                 361.60  154.92   32.10   20.72      437.36      282.32             0.03             0.02    122.79     79.26
4ZpjKfu6Cl8_c_03                 459.85  112.29   63.92   56.92      468.00      416.77             0.03             0.03     48.34     43.05
5milLu-6bWI_c_01                 291.12  174.41   76.54   43.88      409.87      235.00             0.03             0.02     97.84     56.10
5milLu-6bWI_c_02                 232.81  217.06   90.52   41.70      378.80      174.52             3.61             1.66    122.92     56.63
5milLu-6bWI_c_03                 210.77  221.49  126.70   57.21      372.05      167.98             1.30             0.59     93.49     42.21
7YpF6DntOYw_c_01                 376.08  132.86   64.67   48.68      431.49      324.76             0.08             0.06     68.11     51.26
7YpF6DntOYw_c_02                 871.79   54.12   42.50   78.54      460.16      850.33             0.03             0.06     11.58     21.40
7YpF6DntOYw_c_03                 799.37   66.24   42.82   64.65      506.06      764.03             0.03             0.05     23.38     35.30
BCiuXAuCKAU_c_01                 198.72  206.56  171.41   82.98      375.33      181.70             1.62             0.78     33.53     16.23
BCiuXAuCKAU_c_02                 322.20  154.10   82.17   53.32      424.59      275.52             0.45             0.29     71.48     46.39
BCiuXAuCKAU_c_03                 223.36  207.37  129.70   62.55      385.52      185.91             0.64             0.31     77.02     37.14
HKjR70GCRPE_c_01                 318.78  146.56  125.84   85.86      446.48      304.64             0.79             0.54     19.93     13.60
HKjR70GCRPE_c_02                 347.34  145.10   88.30   60.85      447.20      308.19             0.06             0.04     56.74     39.11
HKjR70GCRPE_c_03                 267.01  165.78  129.64   78.20      406.51      245.21             0.03             0.02     36.11     21.78
IKdBLciu_-A_c_01                 372.33  127.60   71.20   55.80      418.68      328.13             0.03             0.02     56.37     44.18
IKdBLciu_-A_c_02                 570.75   95.71   49.36   51.57      499.90      522.32             0.62             0.64     45.74     47.79
IKdBLciu_-A_c_03                 539.26   98.63   46.39   47.03      479.64      486.30             0.03             0.03     52.21     52.94
KHHgQ_Pe4cI_c_01                 236.59  180.62  127.00   70.32      373.72      206.90             0.09             0.05     53.52     29.63
KHHgQ_Pe4cI_c_02                 377.35  119.56   97.32   81.40      428.90      358.75             2.21             1.85     20.03     16.75
KHHgQ_Pe4cI_c_03                 319.65  156.28   90.73   58.05      434.00      277.71             0.03             0.02     65.52     41.93
PmElx9ZVByw_c_01                 342.04  142.53  100.77   70.70      445.76      312.74             0.03             0.02     41.73     29.28
PmElx9ZVByw_c_02                 323.71  147.22  109.70   74.51      439.05      298.22             0.03             0.02     37.49     25.47
PmElx9ZVByw_c_03                 422.91  120.01   84.74   70.61      472.26      393.52             0.03             0.03     35.24     29.36
a5mEmM6w_ks_c_01                 175.44  235.73  174.76   74.13      352.58      149.57             0.05             0.02     60.92     25.84
a5mEmM6w_ks_c_02                 254.64  185.80  107.16   57.67      394.50      212.32             0.04             0.02     78.61     42.31
a5mEmM6w_ks_c_03                 371.04  130.15   89.37   68.66      442.14      339.71             0.03             0.02     40.76     31.31
kMy-6RtoOVU_c_01                 261.12  185.77  108.62   58.47      407.95      219.59             1.47             0.79     75.68     40.74
kMy-6RtoOVU_c_02                 424.29  121.57   76.46   62.89      470.70      387.18             0.03             0.03     45.08     37.08
kMy-6RtoOVU_c_03                 481.80  111.45   55.33   49.65      480.85      431.44             0.06             0.05     56.06     50.30
qrkff49p4E4_c_01                 356.49  147.36   66.98   45.45      444.92      301.94             0.03             0.02     80.35     54.53
qrkff49p4E4_c_02                 343.18  139.38  113.94   81.74      452.89      324.92             0.03             0.02     25.41     18.23
qrkff49p4E4_c_03                 306.35  160.17  101.58   63.42      432.10      269.77             0.03             0.02     58.56     36.56
zC5Fh2tTS1U_c_01                 227.41  193.38  114.52   59.22      360.91      186.63             0.05             0.03     78.81     40.75
zC5Fh2tTS1U_c_02                 288.90  172.54   94.37   54.69      420.29      243.59             0.58             0.34     77.59     44.97
zC5Fh2tTS1U_c_03                 171.91  240.12  188.79   78.62      361.47      150.53            11.33             4.72     39.99     16.66
zR725veL-DI_c_01                 262.62  190.05   93.49   49.19      402.55      211.81             0.36             0.19     96.21     50.62
zR725veL-DI_c_02                 203.89  215.43  153.38   71.20      377.18      175.08             0.36             0.17     61.69     28.64
zR725veL-DI_c_03                 213.68  199.67  165.64   82.96      392.63      196.64             0.06             0.03     33.97     17.01
IzvOYVMltkI_c_01                 328.18  156.71   77.97   49.75      435.56      277.93             0.03             0.02     78.71     50.23
IzvOYVMltkI_c_02                 239.66  191.14  134.23   70.22      401.16      209.88             0.06             0.03     56.86     29.75
IzvOYVMltkI_c_03                 281.87  172.52  105.99   61.43      419.75      243.30             0.03             0.02     66.50     38.55
UrsCy6qIGoo_c_01                 430.53  122.69   66.38   54.11      471.91      384.63             2.35             1.92     53.95     43.98
UrsCy6qIGoo_c_02                 436.48  125.06   46.68   37.32      467.48      373.80             0.30             0.24     78.08     62.44
UrsCy6qIGoo_c_03                 254.61  197.16   90.45   45.88      395.28      200.49             0.20             0.10    106.51     54.02
yn9WN9lsHRE_c_01                 269.58  188.10   85.21   45.30      404.20      214.88             0.06             0.03    102.83     54.67
yn9WN9lsHRE_c_02                 307.62  156.18  113.10   72.42      437.36      280.04             1.29             0.83     41.79     26.76
yn9WN9lsHRE_c_03                 244.70  196.37  103.86   52.89      388.00      197.59             0.12             0.06     92.38     47.05
TOTAL                            308.37 8556.01 5200.62   60.78    23028.39      269.15            34.24             0.40   3321.15     38.82
[1;34mwandb[0m: ğŸš€ View run [33m10_epochs_lr1em4_sbatch_t2[0m at: [34mhttps://wandb.ai/pichenygroup/testruns/runs/y7pkjzix[0m
[1;34mwandb[0m: Find logs at: [1;35mepoch1test/wandb/run-20250114_143457-y7pkjzix/logs[0m
